{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74889845",
   "metadata": {},
   "source": [
    "# Text Classification with LSTM and Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e2268c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting keras\n",
      "  Downloading keras-3.13.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: absl-py in c:\\users\\ernest\\appdata\\roaming\\python\\python312\\site-packages (from keras) (2.4.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (1.26.4)\n",
      "Requirement already satisfied: rich in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (13.7.1)\n",
      "Collecting namex (from keras)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Requirement already satisfied: h5py in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (3.11.0)\n",
      "Collecting optree (from keras)\n",
      "  Downloading optree-0.18.0-cp312-cp312-win_amd64.whl.metadata (35 kB)\n",
      "Collecting ml-dtypes (from keras)\n",
      "  Downloading ml_dtypes-0.5.4-cp312-cp312-win_amd64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from optree->keras) (4.11.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.0)\n",
      "Downloading keras-3.13.2-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 26.6 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.5.4-cp312-cp312-win_amd64.whl (212 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.18.0-cp312-cp312-win_amd64.whl (312 kB)\n",
      "Installing collected packages: namex, optree, ml-dtypes, keras\n",
      "Successfully installed keras-3.13.2 ml-dtypes-0.5.4 namex-0.1.0 optree-0.18.0\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cea68864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.20.0-cp312-cp312-win_amd64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\ernest\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (2.4.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.12.19-py2.py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.7.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.78.0-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\ernest\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (3.13.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\ernest\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Collecting typing_extensions>=3.6.6 (from tensorflow)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: rich in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\ernest\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\ernest\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2026.1.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (12.0.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Downloading tensorflow-2.20.0-cp312-cp312-win_amd64.whl (331.9 MB)\n",
      "   ---------------------------------------- 0.0/331.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 8.7/331.9 MB 53.8 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 21.5/331.9 MB 56.6 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 33.6/331.9 MB 57.6 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 45.1/331.9 MB 56.2 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 56.9/331.9 MB 55.8 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 67.9/331.9 MB 56.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 79.2/331.9 MB 55.5 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 90.4/331.9 MB 55.5 MB/s eta 0:00:05\n",
      "   ----------- --------------------------- 100.7/331.9 MB 54.9 MB/s eta 0:00:05\n",
      "   ------------- ------------------------- 113.8/331.9 MB 55.5 MB/s eta 0:00:04\n",
      "   -------------- ------------------------ 125.6/331.9 MB 55.7 MB/s eta 0:00:04\n",
      "   ---------------- ---------------------- 136.3/331.9 MB 55.5 MB/s eta 0:00:04\n",
      "   ----------------- --------------------- 147.6/331.9 MB 55.5 MB/s eta 0:00:04\n",
      "   ------------------ -------------------- 159.1/331.9 MB 55.3 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 170.4/331.9 MB 55.3 MB/s eta 0:00:03\n",
      "   --------------------- ----------------- 182.2/331.9 MB 55.2 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 194.8/331.9 MB 55.3 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 205.8/331.9 MB 55.3 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 216.0/331.9 MB 55.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 228.1/331.9 MB 55.0 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 240.6/331.9 MB 55.3 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 253.2/331.9 MB 55.6 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 267.1/331.9 MB 56.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 281.5/331.9 MB 56.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 293.3/331.9 MB 56.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 306.4/331.9 MB 57.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 318.2/331.9 MB 56.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  330.3/331.9 MB 57.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  331.9/331.9 MB 57.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  331.9/331.9 MB 57.0 MB/s eta 0:00:01\n",
      "   --------------------------------------- 331.9/331.9 MB 52.0 MB/s eta 0:00:00\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.12.19-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.7.0-py3-none-any.whl (22 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.78.0-cp312-cp312-win_amd64.whl (4.8 MB)\n",
      "   ---------------------------------------- 0.0/4.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 4.8/4.8 MB 57.4 MB/s eta 0:00:00\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 13.4/26.4 MB 70.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.2/26.4 MB 63.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.4/26.4 MB 57.7 MB/s eta 0:00:00\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 5.5/5.5 MB 48.3 MB/s eta 0:00:00\n",
      "Downloading termcolor-3.3.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Installing collected packages: libclang, flatbuffers, typing_extensions, termcolor, tensorboard-data-server, opt_einsum, google_pasta, gast, astunparse, grpcio, tensorboard, tensorflow\n",
      "Successfully installed astunparse-1.6.3 flatbuffers-25.12.19 gast-0.7.0 google_pasta-0.2.0 grpcio-1.78.0 libclang-18.1.1 opt_einsum-3.4.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.3.0 typing_extensions-4.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tensorboard.exe is installed in 'C:\\Users\\ernest\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe and toco.exe are installed in 'C:\\Users\\ernest\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31be8fa0",
   "metadata": {},
   "source": [
    "### Load and pre-process the dataset\n",
    "\n",
    "We will use a dataset that contains over 11,000 tweets associated with disaster keywords like “crash”, “quarantine”, and “bush fires” as well as the location and keyword itself. Every tweet is classified according to whether the tweet really referred to a disaster event or not (a joke with the word or a movie review or something non-disastrous). \n",
    "\n",
    "The classification task consists of determining the label of the tweet (disaster or not) given the text of the tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5919090",
   "metadata": {},
   "source": [
    "We load a pre-trained model for word embedding. We will need it to convert tokens to their ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d510b311",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:551: UserWarning: Adding single vectors to a KeyedVectors which grows by one each time can be costly. Consider adding in batches or preallocating to the required size.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Import model.\n",
    "model_w2v = api.load('glove-wiki-gigaword-300')\n",
    "\n",
    "# Add padding token and embedding to the model, so that we can use it later when we pad the sequences.\n",
    "pad_tok = '<pad>'\n",
    "pad_emb = np.zeros(300)\n",
    "model_w2v.add_vector(pad_tok, pad_emb)\n",
    "pad_tok_id = model_w2v.key_to_index[pad_tok]\n",
    "\n",
    "# Add unknown token and embedding to the model, so that we can use it later when we convert tokens to indices.\n",
    "unk_tok = '<unk>'\n",
    "unk_emb = np.random.normal(size=300)\n",
    "model_w2v.add_vector(unk_tok, unk_emb)\n",
    "unk_tok_id = model_w2v.key_to_index['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e94fbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "filename =  'data/tweets.csv'\n",
    "max_len = 20\n",
    "test_size = 0.2\n",
    "\n",
    "# Load the data. Drop the columns that we won't use.\n",
    "df = pd.read_csv(filename)\n",
    "df.drop(['id','keyword','location'], axis=1, inplace=True)\n",
    "\n",
    "# Extract the text and labels from the dataframe.\t\t\n",
    "text = df['text'].values\n",
    "labels = df['target'].values\n",
    "\n",
    "# Split the data into training and test sets.\t\n",
    "text_train,text_test, labels_train, labels_test = train_test_split(text, labels, test_size=test_size)\n",
    "\n",
    "# Tokenize the text using the NLTK TweetTokenizer. \n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "tokens_train = [tokenizer.tokenize(sentence) for sentence in text_train]\n",
    "tokens_test = [tokenizer.tokenize(sentence) for sentence in text_test]\n",
    "\n",
    "# Convert the tokens to indices using the model's vocabulary. If a token is not in the vocabulary, use the index of the unknown token.\n",
    "tokens_ids_train = [[model_w2v.key_to_index.get(token, unk_tok_id) for token in sentence] for sentence in tokens_train]\n",
    "tokens_ids_test = [[model_w2v.key_to_index.get(token, unk_tok_id) for token in sentence] for sentence in tokens_test]\n",
    "    \n",
    "# Pad the sequences to ensure that they all have the same length.\n",
    "x_train = sequence.pad_sequences(tokens_ids_train, maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(tokens_ids_test, maxlen=max_len)\n",
    "\n",
    "# Get the labels for the training and test sets.\n",
    "y_train = labels_train\n",
    "y_test = labels_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97644f6",
   "metadata": {},
   "source": [
    "### Definition of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d27698",
   "metadata": {},
   "source": [
    "We define the classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba739019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TweetClassifier(nn.ModuleList):\n",
    "\n",
    "\t# We provide as parameters the pre-trained word embeddings, the hidden size of the LSTM layer, the number of layers of the LSTM, and the dropout rate. \n",
    "\tdef __init__(self, vectors, hidden_size, num_layers, dropout):\n",
    "\t\tsuper(TweetClassifier, self).__init__()\n",
    "\t\t\n",
    "\t\t# ensure vectors is a tensor\n",
    "\t\tif not torch.is_tensor(vectors):\n",
    "\t\t\tvectors = torch.tensor(vectors)\n",
    "\n",
    "\t\tself.hidden_dim = hidden_size\n",
    "\t\tself.LSTM_layers = num_layers\n",
    "\t\n",
    "        # init embedding layer\n",
    "\t\tself.embedding = nn.Embedding.from_pretrained(embeddings=vectors)\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\t\t# LSTM layer. \n",
    "\n",
    "\t\t# ADD LSTM LAYER HERE. See lstm documentation at https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html for more details. \n",
    "\n",
    "\t\t# Classification layers. The input size is the same as the hidden size of the LSTM, since we are using the output of the last time step of the LSTM as input to the classification layers. The output size is 1, since we are doing binary classification.\n",
    "\t\tself.fc1 = nn.Linear(in_features=hidden_size, out_features=257)\n",
    "\t\tself.fc2 = nn.Linear(257, 1)\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\t# Initialize the hidden and cell states of the LSTM. The shape of the hidden and cell states should be (num_layers, batch_size, hidden_size).\n",
    "\t\th = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim))\n",
    "\t\tc = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim))\n",
    "\t\t\n",
    "\t\ttorch.nn.init.xavier_normal_(h)\n",
    "\t\ttorch.nn.init.xavier_normal_(c)\n",
    "\n",
    "\t\t# Pass the input through the embedding layer, then through the LSTM layer, and finally through the classification layers. \n",
    "\t\t# The output of the LSTM layer is a tuple containing the output of the last time step and the hidden and cell states. We only need the output of the last time step, which is the first element of the tuple. \n",
    "\t\t# We can use this output as input to the classification layers.\n",
    "\t\tout = self.embedding(x)\n",
    "\t\tout, (hidden, cell) = self.lstm(out, (h,c))\n",
    "\t\tout = self.dropout(out)\n",
    "\n",
    "\t\t# Pass the output of the LSTM layer through the classification layers to get the final output\n",
    "\n",
    "\t\t# ADD CODE HERE\n",
    "\n",
    "\t\t\n",
    "\t\treturn out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d4836c",
   "metadata": {},
   "source": [
    "We define the Dataset for the Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39a823a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DatasetMaper(Dataset):\n",
    "\t'''\n",
    "\tHandles batches of dataset\n",
    "\t'''\n",
    "\tdef __init__(self, x, y):\n",
    "\t\tself.x = x\n",
    "\t\tself.y = y\n",
    "\t\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.x)\n",
    "\t\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\treturn self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3287a20e",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7014b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 0.58573, Train accuracy: 0.57291, Test accuracy: 0.58306\n",
      "Epoch: 2, loss: 0.45349, Train accuracy: 0.59852, Test accuracy: 0.65857\n",
      "Epoch: 3, loss: 0.62765, Train accuracy: 0.62824, Test accuracy: 0.72817\n",
      "Epoch: 4, loss: 0.48637, Train accuracy: 0.70345, Test accuracy: 0.73867\n",
      "Epoch: 5, loss: 0.40029, Train accuracy: 0.75156, Test accuracy: 0.73539\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "def compute_accuracy(ground_truth, predictions):\n",
    "\ttrue_positives = 0\n",
    "\ttrue_negatives = 0\n",
    "\t\t\n",
    "\tfor true, pred in zip(ground_truth, predictions):\n",
    "\t\tif (pred > 0.5) and (true == 1):\n",
    "\t\t\ttrue_positives += 1\n",
    "\t\telif (pred < 0.5) and (true == 0):\n",
    "\t\t\ttrue_negatives += 1\n",
    "\t\telse:\n",
    "\t\t\tpass\n",
    "\t\t\t\t\n",
    "\treturn (true_positives+true_negatives) / len(ground_truth)\n",
    "\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "epochs = 5\n",
    "\n",
    "training_set = DatasetMaper(x_train, y_train)\n",
    "test_set = DatasetMaper(x_test, y_test)\n",
    "\t\t\n",
    "loader_training = DataLoader(training_set, batch_size=batch_size)\n",
    "loader_test = DataLoader(test_set)\n",
    "\n",
    "# We pass the pre-trained word embeddings to the model to initialize the embedding layer\n",
    "model = TweetClassifier(vectors=model_w2v.vectors, hidden_size=128, num_layers=2, dropout=0.5)\t\t\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "for epoch in range(epochs):\n",
    "\t\t\t\n",
    "\ttrain_predictions = []\t\t\t\n",
    "\tmodel.train()\n",
    "\tfor x_batch, y_batch in loader_training:\n",
    "\t\tx = x_batch.type(torch.LongTensor)\n",
    "\t\ty = y_batch.type(torch.FloatTensor)\n",
    "\n",
    "\t\ty = torch.unsqueeze(y, 1)\t\t\n",
    "\t\ty_pred = model(x)\n",
    "\n",
    "\t\tloss = F.binary_cross_entropy(y_pred, y)\t\t\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\t\t\t\n",
    "\t\ttrain_predictions += list(y_pred.squeeze().detach().numpy())\n",
    "\t# Evaluate the model on the test set\n",
    "\ttest_predictions = []\n",
    "\tmodel.eval()\n",
    "\twith torch.no_grad():\n",
    "\t\tfor x_batch, y_batch in loader_test:\n",
    "\t\t\tx = x_batch.type(torch.LongTensor)\n",
    "\t\t\ty = y_batch.type(torch.FloatTensor)\t\t\t\n",
    "\t\t\ty_pred = model(x)\n",
    "\t\t\ttest_predictions += list(y_pred.detach().numpy())\n",
    "\t\t\t\t\n",
    "\t\t\t\n",
    "\ttrain_accuracy = compute_accuracy(labels_train, train_predictions)\n",
    "\ttest_accuracy = compute_accuracy(labels_test, test_predictions)\n",
    "\t\t\t\n",
    "\tprint(\"Epoch: %d, loss: %.5f, Train accuracy: %.5f, Test accuracy: %.5f\" % (epoch+1, loss.item(), train_accuracy, test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
