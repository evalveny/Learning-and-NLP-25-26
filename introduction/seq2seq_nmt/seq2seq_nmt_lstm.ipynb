{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ONk0S00BTMx"
      },
      "source": [
        "# Neural Machine Translation with RNNs\n",
        "\n",
        "\n",
        "\n",
        "**Credits**: This notebook is based on code from: https://github.com/keon/seq2seq\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0bHH6J6BxuO"
      },
      "source": [
        "### Basic setup and module imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "52rMtstdORAp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: torchtext==0.6 in c:\\users\\ernest\\appdata\\roaming\\python\\python312\\site-packages (0.6.0)\n",
            "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from torchtext==0.6) (4.66.5)\n",
            "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from torchtext==0.6) (2.32.3)\n",
            "Requirement already satisfied: torch in c:\\users\\ernest\\appdata\\roaming\\python\\python312\\site-packages (from torchtext==0.6) (2.10.0)\n",
            "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchtext==0.6) (1.26.4)\n",
            "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from torchtext==0.6) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in c:\\users\\ernest\\appdata\\roaming\\python\\python312\\site-packages (from torchtext==0.6) (0.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchtext==0.6) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchtext==0.6) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchtext==0.6) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchtext==0.6) (2026.1.4)\n",
            "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch->torchtext==0.6) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ernest\\appdata\\roaming\\python\\python312\\site-packages (from torch->torchtext==0.6) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ernest\\appdata\\roaming\\python\\python312\\site-packages (from torch->torchtext==0.6) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch->torchtext==0.6) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch->torchtext==0.6) (3.1.4)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch->torchtext==0.6) (2024.6.1)\n",
            "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch->torchtext==0.6) (75.1.0)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->torchtext==0.6) (0.4.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch->torchtext==0.6) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch->torchtext==0.6) (2.1.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install torchtext==0.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: spacy in c:\\users\\ernest\\appdata\\roaming\\python\\python312\\site-packages (3.8.11)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\ernest\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\ernest\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\ernest\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\ernest\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\ernest\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\ernest\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\ernest\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\ernest\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\ernest\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in c:\\users\\ernest\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (4.66.5)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\ernest\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2026.1.4)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\ernest\\appdata\\roaming\\python\\python312\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\ernest\\appdata\\roaming\\python\\python312\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: click>=8.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\ernest\\appdata\\roaming\\python\\python312\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (5.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktzgKn9rUB3B"
      },
      "outputs": [],
      "source": [
        "import re,os\n",
        "import math\n",
        "import random\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "from torchtext.data import Field, BucketIterator\n",
        "from torchtext.datasets import Multi30k\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "\n",
        "assert torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the dataset\n",
        "\n",
        "We will be using a dataset with pairs of sentences in German (source) and English (target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6MbUnb70RlwG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting de-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.8.0/de_core_news_sm-3.8.0-py3-none-any.whl (14.6 MB)\n",
            "     ---------------------------------------- 0.0/14.6 MB ? eta -:--:--\n",
            "     ----- ---------------------------------- 2.1/14.6 MB 11.8 MB/s eta 0:00:02\n",
            "     ------------ --------------------------- 4.7/14.6 MB 11.9 MB/s eta 0:00:01\n",
            "     ------------------- -------------------- 7.1/14.6 MB 11.8 MB/s eta 0:00:01\n",
            "     ------------------------- -------------- 9.4/14.6 MB 12.0 MB/s eta 0:00:01\n",
            "     ------------------------------- ------- 11.8/14.6 MB 11.7 MB/s eta 0:00:01\n",
            "     --------------------------------------  14.4/14.6 MB 11.6 MB/s eta 0:00:01\n",
            "     --------------------------------------- 14.6/14.6 MB 11.5 MB/s eta 0:00:00\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     ------- -------------------------------- 2.4/12.8 MB 11.2 MB/s eta 0:00:01\n",
            "     -------------- ------------------------- 4.7/12.8 MB 11.4 MB/s eta 0:00:01\n",
            "     ---------------------- ----------------- 7.1/12.8 MB 11.8 MB/s eta 0:00:01\n",
            "     ----------------------------- ---------- 9.4/12.8 MB 11.7 MB/s eta 0:00:01\n",
            "     ------------------------------------ -- 12.1/12.8 MB 11.8 MB/s eta 0:00:01\n",
            "     --------------------------------------- 12.8/12.8 MB 11.5 MB/s eta 0:00:00\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "!python -m spacy download de_core_news_sm\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will need to download and uncompress the files with the [training set](https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz), [validation set](https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz) and [test set](https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/mmt16_task1_test.tar.gz). Copy these files to a sub-folder `/data` into the current folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO26oSHNOSrW"
      },
      "source": [
        "We build training, validation and test sets \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bvnyNq5qOUWb"
      },
      "outputs": [],
      "source": [
        "def load_dataset(batch_size):\n",
        "    spacy_de = spacy.load('de_core_news_sm')\n",
        "    spacy_en = spacy.load('en_core_web_sm')\n",
        "    url = re.compile('(<url>.*</url>)')\n",
        "\n",
        "    def tokenize_de(text):\n",
        "        return [tok.text for tok in spacy_de.tokenizer(url.sub('@URL@', text))]\n",
        "\n",
        "    def tokenize_en(text):\n",
        "        return [tok.text for tok in spacy_en.tokenizer(url.sub('@URL@', text))]\n",
        "\n",
        "    DE = Field(tokenize=tokenize_de, include_lengths=True,\n",
        "               init_token='<sos>', eos_token='<eos>')\n",
        "    EN = Field(tokenize=tokenize_en, include_lengths=True,\n",
        "               init_token='<sos>', eos_token='<eos>')\n",
        "    train, val, test = Multi30k.splits(path='data/',exts=('.de', '.en'), fields=(DE, EN))\n",
        "    DE.build_vocab(train.src, min_freq=2)\n",
        "    EN.build_vocab(train.trg, max_size=10000)\n",
        "    train_iter, val_iter, test_iter = BucketIterator.splits(\n",
        "            (train, val, test), batch_size=batch_size, repeat=False)\n",
        "    return train_iter, val_iter, test_iter, DE, EN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "F7z71iFyUkUJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[!] preparing dataset...\n",
            "[TRAIN]:907 (dataset:29000)\t[TEST]:32 (dataset:1000)\n",
            "[DE_vocab]:8014 [en_vocab]:10004\n"
          ]
        }
      ],
      "source": [
        "batch_size=32\n",
        "print(\"[!] preparing dataset...\")\n",
        "\n",
        "train_iter, val_iter, test_iter, DE, EN = load_dataset(batch_size)\n",
        "de_size, en_size = len(DE.vocab), len(EN.vocab)\n",
        "print(\"[TRAIN]:%d (dataset:%d)\\t[TEST]:%d (dataset:%d)\"\n",
        "          % (len(train_iter), len(train_iter.dataset),\n",
        "             len(test_iter), len(test_iter.dataset)))\n",
        "print(\"[DE_vocab]:%d [en_vocab]:%d\" % (de_size, en_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIpmX3hHCA3v"
      },
      "source": [
        "### Model definition\n",
        "\n",
        "Our seq2seq model consists of two separate modules:\n",
        "\n",
        "* The **Encoder** is a RNN that outputs some value for every word from the input sentence. For every input word the encoder outputs a vector and a hidden state, and uses the hidden state for the next input word.\n",
        "\n",
        "* The **Decoder** is another RNN that takes the encoder output vector(s) and outputs a sequence of words to create the translation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDTZjna2Ocd5"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embed_size, hidden_size,\n",
        "                 n_layers=1, dropout=0.5):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embed_size = embed_size\n",
        "        self.embed = nn.Embedding(input_size, embed_size)\n",
        "        self.gru = nn.GRU(embed_size, hidden_size, n_layers,\n",
        "                          dropout=dropout, bidirectional=True)\n",
        "\n",
        "    def forward(self, src, hidden=None):\n",
        "        embedded = self.embed(src)\n",
        "        outputs, hidden = self.gru(embedded, hidden)\n",
        "        # sum bidirectional outputs\n",
        "        outputs = (outputs[:, :, :self.hidden_size] +\n",
        "                   outputs[:, :, self.hidden_size:])\n",
        "        return outputs, hidden\n",
        "\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, output_size,\n",
        "                 n_layers=1, dropout=0.2):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embed = nn.Embedding(output_size, embed_size)\n",
        "        self.dropout = nn.Dropout(dropout, inplace=True)\n",
        "        self.gru = nn.GRU(embed_size, hidden_size,\n",
        "                          n_layers, dropout=dropout)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, last_hidden):    \n",
        "        # Get the embedding of the current input word (last output word)\n",
        "        embedded = self.embed(input).unsqueeze(0)  # (1,B,N)\n",
        "        embedded = self.dropout(embedded)\n",
        "        \n",
        "        output, hidden = self.gru(embedded, last_hidden)\n",
        "        output = output.squeeze(0)  # (1,B,N) -> (B,N)\n",
        "        output = self.out(output)   \n",
        "        output = F.log_softmax(output, dim=1)\n",
        "        return output, hidden\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.size(1)\n",
        "        max_len = trg.size(0)\n",
        "        vocab_size = self.decoder.output_size\n",
        "        outputs = Variable(torch.zeros(max_len, batch_size, vocab_size)).cuda()\n",
        "\n",
        "        encoder_output, hidden = self.encoder(src)\n",
        "        # Initialize decoder's input\n",
        "        hidden = hidden[:self.decoder.n_layers]\n",
        "        output = Variable(trg.data[0, :])  # sos\n",
        "        # Autoregressive decoding one token at each time step. We pass the output of the previous time step as the input to the next time step. \n",
        "        # Use teacher forcing with a certain probability to decide whether to use the actual target token as the next input or to use the predicted token from the previous time step.  \n",
        "        # We accumulate all the outputs in the outputs tensor\n",
        "\n",
        "        \n",
        "        # ADD CODE HERE\n",
        "        \n",
        "\n",
        "\n",
        "        return outputs  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W70KqrVRCJUV"
      },
      "source": [
        "### Training and evaluation helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tn7NNfvbVE2z"
      },
      "outputs": [],
      "source": [
        "def train(e, model, optimizer, train_iter, vocab_size, grad_clip, DE, EN):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    pad = EN.vocab.stoi['<pad>']\n",
        "    for b, batch in enumerate(train_iter):\n",
        "        src, len_src = batch.src\n",
        "        trg, len_trg = batch.trg\n",
        "        src, trg = src.cuda(), trg.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg)\n",
        "        loss = F.nll_loss(output[1:].view(-1, vocab_size),\n",
        "                               trg[1:].contiguous().view(-1),\n",
        "                               ignore_index=pad)\n",
        "        loss.backward()\n",
        "        clip_grad_norm_(model.parameters(), grad_clip)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.data.item()\n",
        "\n",
        "        if b % 100 == 0 and b != 0:\n",
        "            total_loss = total_loss / 100\n",
        "            print(\"[%d][loss:%5.2f][pp:%5.2f]\" %\n",
        "                  (b, total_loss, math.exp(total_loss)))\n",
        "            total_loss = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_iKqxa_O_LZ"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, val_iter, vocab_size, DE, EN):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        pad = EN.vocab.stoi['<pad>']\n",
        "        total_loss = 0\n",
        "        for b, batch in enumerate(val_iter):\n",
        "            src, len_src = batch.src\n",
        "            trg, len_trg = batch.trg\n",
        "            src = src.data.cuda()\n",
        "            trg = trg.data.cuda()\n",
        "            output = model(src, trg, teacher_forcing_ratio=0.0)\n",
        "            loss = F.nll_loss(output[1:].view(-1, vocab_size),\n",
        "                                   trg[1:].contiguous().view(-1),\n",
        "                                   ignore_index=pad)\n",
        "            total_loss += loss.data.item()\n",
        "        return total_loss / len(val_iter)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZOU3y6ACPQk"
      },
      "source": [
        "### Model Training\n",
        "\n",
        "In the following two cells we instatiate our seq2seq model and train it for 10 epochs. Notice that for the model to converge we would need to train for almost 100 epochs, but after 10 epochs (20 minutes aprox. with a T4 GPU) it starts to produce reasonable results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "r6gatz1xUvbN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[!] Instantiating models...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ernest\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\rnn.py:1334: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  super().__init__(\"GRU\", *args, **kwargs)\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "Torch not compiled with CUDA enabled",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[42], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m encoder \u001b[38;5;241m=\u001b[39m Encoder(de_size, embed_size, hidden_size, n_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m      8\u001b[0m decoder \u001b[38;5;241m=\u001b[39m Decoder(embed_size, hidden_size, en_size, n_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m seq2seq \u001b[38;5;241m=\u001b[39m Seq2Seq(encoder, decoder)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     10\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(seq2seq\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(seq2seq)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1093\u001b[0m, in \u001b[0;36mModule.cuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m, device: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m device \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   1077\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m \n\u001b[0;32m   1079\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1093\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: t\u001b[38;5;241m.\u001b[39mcuda(device))\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:933\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    932\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 933\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[0;32m    935\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_subclasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfake_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n\u001b[0;32m    937\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:933\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    932\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 933\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[0;32m    935\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_subclasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfake_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n\u001b[0;32m    937\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:964\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    960\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    961\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    962\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    963\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 964\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[0;32m    965\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    967\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1093\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m, device: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m device \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   1077\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m \n\u001b[0;32m   1079\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1093\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: t\u001b[38;5;241m.\u001b[39mcuda(device))\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\cuda\\__init__.py:417\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    413\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    414\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    415\u001b[0m     )\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    419\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    420\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    421\u001b[0m     )\n",
            "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
          ]
        }
      ],
      "source": [
        "lr=0.0001\n",
        "hidden_size = 512\n",
        "embed_size = 256\n",
        "\n",
        "\n",
        "print(\"[!] Instantiating models...\")\n",
        "encoder = Encoder(de_size, embed_size, hidden_size, n_layers=2, dropout=0.5)\n",
        "decoder = Decoder(embed_size, hidden_size, en_size, n_layers=1, dropout=0.5)\n",
        "seq2seq = Seq2Seq(encoder, decoder).cuda()\n",
        "optimizer = optim.Adam(seq2seq.parameters(), lr=lr)\n",
        "print(seq2seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2UcMG9oPz6P"
      },
      "outputs": [],
      "source": [
        "epochs=10\n",
        "grad_clip=10.0\n",
        "\n",
        "best_val_loss = None\n",
        "\n",
        "for e in range(1, epochs+1):\n",
        "\n",
        "    train(e, seq2seq, optimizer, train_iter, en_size, grad_clip, DE, EN)\n",
        "    val_loss = evaluate(seq2seq, val_iter, en_size, DE, EN)\n",
        "\n",
        "    print(\"[Epoch:%d] val_loss:%5.3f | val_pp:%5.2fS\"\n",
        "          % (e, val_loss, math.exp(val_loss)))\n",
        "\n",
        "    # Save the model if the validation loss is the best we've seen so far.\n",
        "    if not best_val_loss or val_loss < best_val_loss:\n",
        "        print(\"[!] saving model...\")\n",
        "        if not os.path.isdir(\".save\"):\n",
        "            os.makedirs(\".save\")\n",
        "        torch.save(seq2seq.state_dict(), 'seq2seq_%d.pt' % (e))\n",
        "        best_val_loss = val_loss\n",
        "\n",
        "test_loss = evaluate(seq2seq, test_iter, en_size, DE, EN)\n",
        "print(\"[TEST] loss:%5.2f\" % test_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EmFPUerCUKc"
      },
      "source": [
        "### Qualitative Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDqT0q87ka56"
      },
      "outputs": [],
      "source": [
        "for b, batch in enumerate(test_iter):\n",
        "  src, len_src = batch.src\n",
        "  trg, len_trg = batch.trg\n",
        "  src = src.data.cuda()\n",
        "  trg = trg.data.cuda()\n",
        "  output, weights_matrix = seq2seq(src, trg, teacher_forcing_ratio=0.0)\n",
        "\n",
        "  #visualize 5 predictions\n",
        "  for n in range(5):\n",
        "    s_src = [DE.vocab.itos[w] for w in src[:,n]]\n",
        "    print('SOURCE:    '+' '.join([w for w in s_src if w not in ['<sos>','<pad>','<eos>','<unk>']]))\n",
        "\n",
        "    s_trg = [EN.vocab.itos[w] for w in trg[:,n]]\n",
        "    print('TARGET:    '+' '.join([w for w in s_trg if w not in ['<sos>','<pad>','<eos>','<unk>']]))\n",
        "\n",
        "    s_pred = [EN.vocab.itos[torch.argmax(w)] for w in output[:,n,:]]\n",
        "    print('PREDICTED: '+' '.join([w for w in s_pred if w not in ['<sos>','<pad>','<eos>','<unk>']]))\n",
        "\n",
        "    print('-----------------')\n",
        "\n",
        "  break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
